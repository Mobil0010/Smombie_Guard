import cv2
import mediapipe as mp
import time
import winsound
import numpy as np
from collections import deque
from ultralytics import YOLO
import head_pose_utils as utils 

# ==========================================
# 1. ì„¤ì • (íŠœë‹ëœ ê°’ ì ìš©)
# ==========================================
print("ğŸš€ ìŠ¤ëª¸ë¹„ ê°€ë“œ (ìµœì í™” ë²„ì „) ê°€ë™ ì¤‘...")

# ğŸŒŸ [ìˆ˜ì • 1] ì†ë„ í–¥ìƒì„ ìœ„í•´ ê°€ë²¼ìš´ ëª¨ë¸(Nano)ë¡œ êµì²´!
# m(medium) -> n(nano) : ë°˜ì‘ ì†ë„ê°€ í›¨ì”¬ ë¹¨ë¼ì§
model = YOLO('yolov8m.pt') 
target_classes = [0, 67, 73] 

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)

mp_pose = mp.solutions.pose
pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)

cap = cv2.VideoCapture(0)
if not cap.isOpened(): exit()

shoulder_history = deque(maxlen=20)

# ğŸŒŸ [ìˆ˜ì • 2] ê±·ê¸° ê°ì§€ ê¸°ì¤€ ëŒ€í­ ì™„í™” (0.005 -> 0.02)
# ì´ì œ ì–´ê¹¨ë¥¼ ê½¤ í¬ê²Œ ì›€ì§ì—¬ì•¼ 'ê±·ê¸°'ë¡œ ì¸ì‹í•¨ (ìˆ¨ì‰¬ê¸°ëŠ” ë¬´ì‹œ)
WALKING_THRESHOLD = 0.02 
last_beep_time = 0

base_pitch = 0 
base_neck = 90
base_chin_dist = 0.5 

print("âœ… ê°ì‹œ ì‹œì‘! (ë°˜ì‘ ì†ë„ UP, ê¸°ì¤€ ì™„í™”)")

while True:
    ret, frame = cap.read()
    if not ret: break

    h, w, _ = frame.shape # ì—ëŸ¬ ë°©ì§€ìš© ìœ„ì¹˜

    # ----------------------------------
    # ğŸŒŸ [Step 0] ì•¼ê°„ ëª¨ë“œ ì²´í¬ (ê¸°ì¤€ ë³€ê²½)
    # ----------------------------------
    brightness = utils.calculate_brightness(frame)
    
    # ğŸŒŸ [ìˆ˜ì • 3] ë‚˜ì´íŠ¸ ëª¨ë“œ ê¸°ì¤€ ë‚®ì¶¤ (80 -> 50)
    # ì´ì œ ì›¬ë§Œí•œ ì‹¤ë‚´ ì¡°ëª…ì—ì„œëŠ” ì•ˆ ì¼œì§. ì§„ì§œ ì–´ë‘ìš¸ ë•Œë§Œ ì¼œì§.
    if brightness < 50: 
        enhanced_frame = utils.apply_night_vision(frame, gamma=2.0)
        cv2.putText(enhanced_frame, f"NIGHT MODE (Bright:{int(brightness)})", (10, h - 50), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    else:
        enhanced_frame = frame.copy() 

    img_rgb = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2RGB)
    
    # ----------------------------------
    # AI ëª¨ë¸ ì‹¤í–‰
    # ----------------------------------
    face_results = face_mesh.process(img_rgb)
    pose_results = pose.process(img_rgb)
    
    is_looking_down = False
    is_phone_detected = False
    is_walking = False
    debug_text = ""

    # A. ê³ ê°œ ìˆ™ì„ ê°ì§€
    score = 0 
    if pose_results.pose_landmarks:
        neck = utils.get_neck_angle(pose_results.pose_landmarks.landmark, w, h)
        if abs(neck - base_neck) > 20: 
            score += 1
            debug_text += "Neck "
        low_angle = utils.check_low_angle_status(pose_results.pose_landmarks.landmark, w, h)
        if low_angle > 20: 
            score += 1
            debug_text += "LowAngle "

    if face_results.multi_face_landmarks:
        for fl in face_results.multi_face_landmarks:
            pitch, _ = utils.get_head_pose(enhanced_frame, fl.landmark)
            if (pitch - base_pitch) > 15: 
                score += 1
                debug_text += "Pitch "
            if pose_results.pose_landmarks:
                chin_dist = utils.get_chin_shoulder_distance(fl.landmark, pose_results.pose_landmarks.landmark, w, h)
                if chin_dist < (base_chin_dist * 0.8): 
                    score += 1
                    debug_text += "ChinDist "

    if score > 0:
        is_looking_down = True
        cv2.putText(enhanced_frame, f"HEAD DOWN: {debug_text}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
    else:
        cv2.putText(enhanced_frame, "HEAD UP", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # B. ê±·ê¸° ê°ì§€ (ëœ ì˜ˆë¯¼í•˜ê²Œ)
    if pose_results.pose_landmarks:
        landmarks = pose_results.pose_landmarks.landmark
        avg_shoulder_y = (landmarks[11].y + landmarks[12].y) / 2
        shoulder_history.append(avg_shoulder_y)
        
        if len(shoulder_history) >= 10:
            # ì§„í­(amplitude)ì´ 0.02 ì´ìƒì´ì–´ì•¼ ê±·ëŠ” ê²ƒìœ¼ë¡œ ì¸ì •
            if (max(shoulder_history) - min(shoulder_history)) > WALKING_THRESHOLD:
                is_walking = True
                cv2.putText(enhanced_frame, "WALKING", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            else:
                cv2.putText(enhanced_frame, "STANDING", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 100, 100), 2)

    # C. í° ê°ì§€
    detect_conf = 0.25 if brightness < 50 else 0.35
    yolo_results = model(enhanced_frame, classes=target_classes, conf=detect_conf, verbose=False)
    for r in yolo_results:
        for box in r.boxes:
            if int(box.cls[0]) != 0: 
                is_phone_detected = True
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                cv2.rectangle(enhanced_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)

    # D. ìµœì¢… ê²½ê³ 
    if is_phone_detected and is_looking_down and is_walking:
        cv2.rectangle(enhanced_frame, (0,0), (w, h), (0,0,255), 10)
        cv2.putText(enhanced_frame, "SMOMBIE DETECTED!", (w//2 - 200, h//2), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 4)
        if time.time() - last_beep_time > 1.0:
            winsound.Beep(1000, 500)
            last_beep_time = time.time()

    cv2.imshow('Optimized Guard', enhanced_frame)
    
    key = cv2.waitKey(1)
    if key == ord('q'): break
    elif key == ord(' '): 
        bp, bn, bc = utils.calibrate_current(enhanced_frame, face_results, pose_results)
        base_pitch = bp
        base_neck = bn
        base_chin_dist = bc
        print(f"ğŸ¯ ë³´ì • ì™„ë£Œ!")
        cv2.rectangle(enhanced_frame, (0,0), (w, h), (255, 255, 0), -1)
        cv2.imshow('Optimized Guard', enhanced_frame)
        cv2.waitKey(100)

cap.release()
cv2.destroyAllWindows()